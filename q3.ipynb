{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# import sys\n",
    "# sys.path.insert(0, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape=(2682, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>845974102619906048</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Democrats are smiling in D.C. that the Freedom...</td>\n",
       "      <td>2017-03-26 15:21:58</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>846166053663191040</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>General Kelly is doing a great job at the bord...</td>\n",
       "      <td>2017-03-27 04:04:42</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>835814988686233601</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>The race for DNC Chairman was, of course, tota...</td>\n",
       "      <td>2017-02-26 13:33:16</td>\n",
       "      <td>android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>835817351178301440</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>For first time the failing @nytimes will take ...</td>\n",
       "      <td>2017-02-26 13:42:39</td>\n",
       "      <td>android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>835916511944523777</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Russia talk is FAKE NEWS put out by the Dems, ...</td>\n",
       "      <td>2017-02-26 20:16:41</td>\n",
       "      <td>android</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id           handle  \\\n",
       "0  845974102619906048  realDonaldTrump   \n",
       "1  846166053663191040  realDonaldTrump   \n",
       "2  835814988686233601  realDonaldTrump   \n",
       "3  835817351178301440  realDonaldTrump   \n",
       "4  835916511944523777  realDonaldTrump   \n",
       "\n",
       "                                               tweet                 date  \\\n",
       "0  Democrats are smiling in D.C. that the Freedom...  2017-03-26 15:21:58   \n",
       "1  General Kelly is doing a great job at the bord...  2017-03-27 04:04:42   \n",
       "2  The race for DNC Chairman was, of course, tota...  2017-02-26 13:33:16   \n",
       "3  For first time the failing @nytimes will take ...  2017-02-26 13:42:39   \n",
       "4  Russia talk is FAKE NEWS put out by the Dems, ...  2017-02-26 20:16:41   \n",
       "\n",
       "    device  \n",
       "0   iphone  \n",
       "1   iphone  \n",
       "2  android  \n",
       "3  android  \n",
       "4  android  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train_data_for_students.tsv', sep='\\t', header=None)\n",
    "df.columns = ['id', 'handle', 'tweet', 'date', 'device']\n",
    "print(f'{df.shape=}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "android                                                                                1683\n",
       "iphone                                                                                  755\n",
       "<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>                      201\n",
       "<a href=\"http://www.twitter.com\" rel=\"nofollow\">Twitter for BlackBerry</a>               13\n",
       "<a href=\"https://about.twitter.com/products/tweetdeck\" rel=\"nofollow\">TweetDeck</a>       9\n",
       "<a href=\"http://twitter.com/#!/download/ipad\" rel=\"nofollow\">Twitter for iPad</a>         4\n",
       "<a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>                               3\n",
       "<a href=\"https://periscope.tv\" rel=\"nofollow\">Periscope.TV</a>                            2\n",
       "<a href=\"http://www.facebook.com/twitter\" rel=\"nofollow\">Facebook</a>                     1\n",
       "Name: device, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.device.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is issue with the label column. There are values that not iphone / android. WTF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "android    1683\n",
       "iphone      755\n",
       "Name: device, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[(df.device == 'iphone') | (df.device == 'android')]\n",
    "df.device.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add numric label column\n",
    "# android = 1\n",
    "# iphone = 0\n",
    "\n",
    "df['label'] = 0\n",
    "df.loc[df['device'] == 'android', 'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input df shape : (2438, 6)\n",
      "Number of folds: 3, total samples (after removing NaN): 2438\n",
      "fold: 0, num samples: 813\n",
      "fold: 1, num samples: 813\n",
      "fold: 2, num samples: 812\n"
     ]
    }
   ],
   "source": [
    "from data_processing import create_folds\n",
    "\n",
    "# Using StratifiedKfold since the label is not that balanced\n",
    "\n",
    "df = create_folds(df, label_name='device', num_folds=3, seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import preprocess\n",
    "\n",
    "# remove urls from tweets snice all hte urls with tweeter shortener\n",
    "\n",
    "df.tweet = df.tweet.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-21fd37ae2f0e743e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\itama\\.cache\\huggingface\\datasets\\csv\\default-21fd37ae2f0e743e\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a834fe761d2e458a98f99f6e2d68bd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991cbd940792478495683dac4c154a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\itama\\.cache\\huggingface\\datasets\\csv\\default-21fd37ae2f0e743e\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13871f7fcb540f78d7085dfd3c5a398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_processing import create_datasetdict\n",
    "\n",
    "dataset = create_datasetdict(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94fc58f733143b7b523ec12826a1f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2438 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\itama\\Google Drive\\לימודים\\תואר שני\\Advanced machine learning\\aml-hw4\\cs3603-Advance-Machine-Learning-hw4\\q3.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/q3.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m tokenize_function\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/q3.ipynb#ch0000007?line=2'>3</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mdistilbert-base-uncased\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/q3.ipynb#ch0000007?line=5'>6</a>\u001b[0m tokenized_datasets \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(tokenize_function, tokenizer)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\datasets\\dataset_dict.py:438\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=434'>435</a>\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=435'>436</a>\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=436'>437</a>\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=437'>438</a>\u001b[0m     {\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=438'>439</a>\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=439'>440</a>\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=440'>441</a>\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=441'>442</a>\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=442'>443</a>\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=443'>444</a>\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=444'>445</a>\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=445'>446</a>\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=446'>447</a>\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=447'>448</a>\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=448'>449</a>\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=449'>450</a>\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=450'>451</a>\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=451'>452</a>\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=452'>453</a>\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=453'>454</a>\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=454'>455</a>\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=455'>456</a>\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=456'>457</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=457'>458</a>\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=458'>459</a>\u001b[0m     }\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=459'>460</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\datasets\\dataset_dict.py:439\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=434'>435</a>\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=435'>436</a>\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=436'>437</a>\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=437'>438</a>\u001b[0m     {\n\u001b[1;32m--> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=438'>439</a>\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=439'>440</a>\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=440'>441</a>\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=441'>442</a>\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=442'>443</a>\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=443'>444</a>\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=444'>445</a>\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=445'>446</a>\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=446'>447</a>\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=447'>448</a>\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=448'>449</a>\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=449'>450</a>\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=450'>451</a>\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=451'>452</a>\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=452'>453</a>\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=453'>454</a>\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=454'>455</a>\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=455'>456</a>\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=456'>457</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=457'>458</a>\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=458'>459</a>\u001b[0m     }\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/dataset_dict.py?line=459'>460</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\datasets\\arrow_dataset.py:1955\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1951'>1952</a>\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1953'>1954</a>\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1954'>1955</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1955'>1956</a>\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1956'>1957</a>\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1957'>1958</a>\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1958'>1959</a>\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1959'>1960</a>\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1960'>1961</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1961'>1962</a>\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1962'>1963</a>\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1963'>1964</a>\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1964'>1965</a>\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1965'>1966</a>\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1966'>1967</a>\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1967'>1968</a>\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1968'>1969</a>\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1969'>1970</a>\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1970'>1971</a>\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1971'>1972</a>\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1972'>1973</a>\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1973'>1974</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1974'>1975</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1976'>1977</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\datasets\\arrow_dataset.py:520\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=517'>518</a>\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=518'>519</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=519'>520</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=520'>521</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=521'>522</a>\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=522'>523</a>\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\datasets\\arrow_dataset.py:487\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=479'>480</a>\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=480'>481</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=481'>482</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=482'>483</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=483'>484</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=484'>485</a>\u001b[0m }\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=485'>486</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=486'>487</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=487'>488</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=488'>489</a>\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\datasets\\fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/fingerprint.py?line=451'>452</a>\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/fingerprint.py?line=452'>453</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/fingerprint.py?line=453'>454</a>\u001b[0m             )\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/fingerprint.py?line=455'>456</a>\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/fingerprint.py?line=457'>458</a>\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/fingerprint.py?line=459'>460</a>\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/fingerprint.py?line=461'>462</a>\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\datasets\\arrow_dataset.py:2320\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=2317'>2318</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched:\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=2318'>2319</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pbar):\n\u001b[1;32m-> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=2319'>2320</a>\u001b[0m         example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=2320'>2321</a>\u001b[0m         \u001b[39mif\u001b[39;00m update_data:\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=2321'>2322</a>\u001b[0m             \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\datasets\\arrow_dataset.py:2220\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=2217'>2218</a>\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=2218'>2219</a>\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[1;32m-> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=2219'>2220</a>\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=2220'>2221</a>\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=2221'>2222</a>\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=2222'>2223</a>\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\datasets\\arrow_dataset.py:1915\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[1;34m(item, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1910'>1911</a>\u001b[0m decorated_item \u001b[39m=\u001b[39m (\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1911'>1912</a>\u001b[0m     Example(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched \u001b[39melse\u001b[39;00m Batch(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1912'>1913</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1913'>1914</a>\u001b[0m \u001b[39m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1914'>1915</a>\u001b[0m result \u001b[39m=\u001b[39m f(decorated_item, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1915'>1916</a>\u001b[0m \u001b[39m# Return a standard dict\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/datasets/arrow_dataset.py?line=1916'>1917</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, LazyDict) \u001b[39melse\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\itama\\Google Drive\\לימודים\\תואר שני\\Advanced machine learning\\aml-hw4\\cs3603-Advance-Machine-Learning-hw4\\model.py:7\u001b[0m, in \u001b[0;36mtokenize_function\u001b[1;34m(examples, tokenizer)\u001b[0m\n\u001b[0;32m      <a href='file:///c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/model.py?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_function\u001b[39m(examples, tokenizer):\n\u001b[1;32m----> <a href='file:///c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/model.py?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(examples[\u001b[39m\"\u001b[39;49m\u001b[39mtweet\u001b[39;49m\u001b[39m\"\u001b[39;49m], padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "from model import tokenize_function\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_id = 2\n",
    "e = tokenizer.encode_plus(df.loc[sample_id, 'tweet'], padding=\"max_length\", truncation=True)\n",
    "e['labels'] = [df.loc[sample_id, 'label']]\n",
    "e.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweet', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1625\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tweet', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 813\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"tweet\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(tokenized_datasets['test'], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1625\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2944a46bab4111864bcb5049d4b38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "model.eval()\n",
    "acc = 0\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    y_pred = torch.argmax(outputs.logits, dim=-1)  # using .logits to change it to Tensor\n",
    "    y = batch['labels']\n",
    "    acc += sum(y_pred == y).item()\n",
    "\n",
    "acc = acc / len(tokenized_datasets['test'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.915129151291513"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([8]) <class 'torch.Tensor'>\n",
      "input_ids torch.Size([8, 512]) <class 'torch.Tensor'>\n",
      "attention_mask torch.Size([8, 512]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for batch in eval_dataloader:\n",
    "    \n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    for k, v in batch.items():\n",
    "        print(k, v.shape, type(v))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet    I am now going to the brand new Trump Internat...\n",
       "label                                                    1\n",
       "Name: 11, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_id = 11\n",
    "e = tokenizer.encode_plus(df.loc[sample_id, 'tweet'], padding=\"max_length\", truncation=True)\n",
    "e['labels'] = [df.loc[sample_id, 'label']]\n",
    "df.iloc[sample_id][['tweet', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([1, 512]) <class 'torch.Tensor'>\n",
      "attention_mask torch.Size([1, 512]) <class 'torch.Tensor'>\n",
      "labels torch.Size([1]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# batch = {k: torch.tensor(v, dtype=torch.int32, device=device) for k, v in e.items()}\n",
    "batch = {k: torch.tensor(v).to(device) for k, v in e.items()}\n",
    "\n",
    "# batch['labels'] = batch['labels'].type(torch.LongTensor).to(device)\n",
    "batch['attention_mask'] = batch['attention_mask'].unsqueeze(0)\n",
    "batch['input_ids'] = batch['input_ids'].unsqueeze(0)\n",
    "\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape, type(v))\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)    \n",
    "y_pred = torch.argmax(outputs.logits, dim=-1)  # using .logits to change it to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "def predict_one(sample_id: int, model: transformers.models) -> int:\n",
    "    \"\"\"\n",
    "    given sample_id from the original df and trained model, the fuction predict the label are return it.\n",
    "    \"\"\"\n",
    "    # tokenizer the sample    sample = tokenizer.encode_plus(df.loc[sample_id, 'tweet'], padding=\"max_length\", truncation=True)\n",
    "    sample['labels'] = [df.loc[sample_id, 'label']]\n",
    "\n",
    "    # convert all the sample data to tensors\n",
    "    sample = {k: torch.tensor(v).to(device) for k, v in e.items()}\n",
    "    sample['attention_mask'] = sample['attention_mask'].unsqueeze(0)\n",
    "    sample['input_ids'] = sample['input_ids'].unsqueeze(0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**sample)\n",
    "\n",
    "    y_pred = torch.argmax(outputs.logits, dim=-1)  # using .logits to change it to Tensor\n",
    "    return y_pred.item()\n",
    "\n",
    "predict_one(11, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e903c692405e917c7fbf1a2efa503212707a7d4a30a85b3348439c72c8274b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('aml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
