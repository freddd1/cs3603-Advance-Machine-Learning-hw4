{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "# import sys\n",
    "# sys.path.insert(0, \".\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape=(2682, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>845974102619906048</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Democrats are smiling in D.C. that the Freedom...</td>\n",
       "      <td>2017-03-26 15:21:58</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>846166053663191040</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>General Kelly is doing a great job at the bord...</td>\n",
       "      <td>2017-03-27 04:04:42</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>835814988686233601</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>The race for DNC Chairman was, of course, tota...</td>\n",
       "      <td>2017-02-26 13:33:16</td>\n",
       "      <td>android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>835817351178301440</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>For first time the failing @nytimes will take ...</td>\n",
       "      <td>2017-02-26 13:42:39</td>\n",
       "      <td>android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>835916511944523777</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Russia talk is FAKE NEWS put out by the Dems, ...</td>\n",
       "      <td>2017-02-26 20:16:41</td>\n",
       "      <td>android</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id           handle  \\\n",
       "0  845974102619906048  realDonaldTrump   \n",
       "1  846166053663191040  realDonaldTrump   \n",
       "2  835814988686233601  realDonaldTrump   \n",
       "3  835817351178301440  realDonaldTrump   \n",
       "4  835916511944523777  realDonaldTrump   \n",
       "\n",
       "                                               tweet                 date  \\\n",
       "0  Democrats are smiling in D.C. that the Freedom...  2017-03-26 15:21:58   \n",
       "1  General Kelly is doing a great job at the bord...  2017-03-27 04:04:42   \n",
       "2  The race for DNC Chairman was, of course, tota...  2017-02-26 13:33:16   \n",
       "3  For first time the failing @nytimes will take ...  2017-02-26 13:42:39   \n",
       "4  Russia talk is FAKE NEWS put out by the Dems, ...  2017-02-26 20:16:41   \n",
       "\n",
       "    device  \n",
       "0   iphone  \n",
       "1   iphone  \n",
       "2  android  \n",
       "3  android  \n",
       "4  android  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train_data_for_students.tsv', sep='\\t', header=None)\n",
    "df.columns = ['id', 'handle', 'tweet', 'date', 'device']\n",
    "print(f'{df.shape=}')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "android                                                                                1683\n",
       "iphone                                                                                  755\n",
       "<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>                      201\n",
       "<a href=\"http://www.twitter.com\" rel=\"nofollow\">Twitter for BlackBerry</a>               13\n",
       "<a href=\"https://about.twitter.com/products/tweetdeck\" rel=\"nofollow\">TweetDeck</a>       9\n",
       "<a href=\"http://twitter.com/#!/download/ipad\" rel=\"nofollow\">Twitter for iPad</a>         4\n",
       "<a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>                               3\n",
       "<a href=\"https://periscope.tv\" rel=\"nofollow\">Periscope.TV</a>                            2\n",
       "<a href=\"http://www.facebook.com/twitter\" rel=\"nofollow\">Facebook</a>                     1\n",
       "Name: device, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.device.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is issue with the label column. There are values that not iphone / android. WTF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "android    1683\n",
       "iphone      755\n",
       "Name: device, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[(df.device == 'iphone') | (df.device == 'android')]\n",
    "df.device.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add numric label column\n",
    "# android = 1\n",
    "# iphone = 0\n",
    "\n",
    "df['label'] = 0\n",
    "df.loc[df['device'] == 'android', 'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input df shape : (2438, 6)\n",
      "Number of folds: 3, total samples (after removing NaN): 2438\n",
      "fold: 0, num samples: 813\n",
      "fold: 1, num samples: 813\n",
      "fold: 2, num samples: 812\n"
     ]
    }
   ],
   "source": [
    "from data_processing import create_folds\n",
    "\n",
    "# Using StratifiedKfold since the label is not that balanced\n",
    "\n",
    "NUMBER_OF_FOLDS = 3\n",
    "\n",
    "df = create_folds(df, label_name='device', num_folds=NUMBER_OF_FOLDS, seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import preprocess\n",
    "\n",
    "# remove urls from tweets snice all hte urls with tweeter shortener\n",
    "\n",
    "df.tweet = df.tweet.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919870f2a1494abb8bab7d5431499d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV steps:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default-44ce9edaa6d9ad28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\itama\\.cache\\huggingface\\datasets\\csv\\default-44ce9edaa6d9ad28\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3920ddf721194ba789eab3b88b322da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8542998b5f47e5a14b1870e5ad12a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\itama\\.cache\\huggingface\\datasets\\csv\\default-44ce9edaa6d9ad28\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21bbc0254fb4727b6cbf5d744bec03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9faf3169b744b5ac9732af1665c163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1625 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfaef33ef2b48cbb30b1661da00a566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/813 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d87d967750459ca84c06ca76e97678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EPOCHS:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf74f81382574cb7811c1a1c626ae74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\itama\\Google Drive\\לימודים\\תואר שני\\Advanced machine learning\\aml-hw4\\cs3603-Advance-Machine-Learning-hw4\\q3.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/q3.ipynb#ch0000051?line=19'>20</a>\u001b[0m optimizer \u001b[39m=\u001b[39m AdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr, weight_decay\u001b[39m=\u001b[39mwd)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/q3.ipynb#ch0000051?line=21'>22</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(data\u001b[39m=\u001b[39mdf,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/q3.ipynb#ch0000051?line=22'>23</a>\u001b[0m                 tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/q3.ipynb#ch0000051?line=23'>24</a>\u001b[0m                 model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/q3.ipynb#ch0000051?line=28'>29</a>\u001b[0m                 num_epochs\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/q3.ipynb#ch0000051?line=29'>30</a>\u001b[0m                 output_name\u001b[39m=\u001b[39moutput_name)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/q3.ipynb#ch0000051?line=33'>34</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/q3.ipynb#ch0000051?line=34'>35</a>\u001b[0m trainer\u001b[39m.\u001b[39mfinal_eval()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/q3.ipynb#ch0000051?line=35'>36</a>\u001b[0m df\u001b[39m.\u001b[39mloc[:, \u001b[39m'\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mloc[:, \u001b[39m'\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\itama\\Google Drive\\לימודים\\תואר שני\\Advanced machine learning\\aml-hw4\\cs3603-Advance-Machine-Learning-hw4\\model.py:94\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/model.py?line=91'>92</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_epochs), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEPOCHS\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     <a href='file:///c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/model.py?line=92'>93</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='file:///c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/model.py?line=93'>94</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch()\n\u001b[0;32m     <a href='file:///c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/model.py?line=95'>96</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[0;32m     <a href='file:///c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/model.py?line=96'>97</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_epoch()\n",
      "File \u001b[1;32mc:\\Users\\itama\\Google Drive\\לימודים\\תואר שני\\Advanced machine learning\\aml-hw4\\cs3603-Advance-Machine-Learning-hw4\\model.py:134\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/model.py?line=131'>132</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain epoch\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/model.py?line=132'>133</a>\u001b[0m     batch \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m--> <a href='file:///c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/model.py?line=133'>134</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/model.py?line=134'>135</a>\u001b[0m     loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/Google%20Drive/%D7%9C%D7%99%D7%9E%D7%95%D7%93%D7%99%D7%9D/%D7%AA%D7%95%D7%90%D7%A8%20%D7%A9%D7%A0%D7%99/Advanced%20machine%20learning/aml-hw4/cs3603-Advance-Machine-Learning-hw4/model.py?line=136'>137</a>\u001b[0m     epoch_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:729\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=720'>721</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=721'>722</a>\u001b[0m \u001b[39mlabels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=722'>723</a>\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=723'>724</a>\u001b[0m \u001b[39m    config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=724'>725</a>\u001b[0m \u001b[39m    If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=725'>726</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=726'>727</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m--> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=728'>729</a>\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=729'>730</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=730'>731</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=731'>732</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=732'>733</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=733'>734</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=734'>735</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=735'>736</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=736'>737</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=737'>738</a>\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=738'>739</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:551\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=548'>549</a>\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=549'>550</a>\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=550'>551</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=551'>552</a>\u001b[0m     x\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=552'>553</a>\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=553'>554</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=554'>555</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=555'>556</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=556'>557</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=557'>558</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:327\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=323'>324</a>\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=324'>325</a>\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[1;32m--> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=326'>327</a>\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=327'>328</a>\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=328'>329</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=329'>330</a>\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=331'>332</a>\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:288\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=285'>286</a>\u001b[0m \u001b[39m# Feed Forward Network\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=286'>287</a>\u001b[0m ffn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(sa_output)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=287'>288</a>\u001b[0m ffn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer_norm(ffn_output \u001b[39m+\u001b[39;49m sa_output)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=289'>290</a>\u001b[0m output \u001b[39m=\u001b[39m (ffn_output,)\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=290'>291</a>\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/normalization.py?line=187'>188</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/normalization.py?line=188'>189</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[0;32m    <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/modules/normalization.py?line=189'>190</a>\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aml\\lib\\site-packages\\torch\\nn\\functional.py:2486\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/functional.py?line=2481'>2482</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/functional.py?line=2482'>2483</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/functional.py?line=2483'>2484</a>\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[0;32m   <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/functional.py?line=2484'>2485</a>\u001b[0m     )\n\u001b[1;32m-> <a href='file:///c%3A/Users/itama/miniconda3/envs/aml/lib/site-packages/torch/nn/functional.py?line=2485'>2486</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from model import Trainer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "lr_optios = [5e-5, 5e-4]\n",
    "weight_decay_optios = [5e-3, 1e-2, 2e-2]\n",
    "\n",
    "preds = df[['id', 'label', 'fold']]\n",
    "total = len(lr_optios)*len(weight_decay_optios)*NUMBER_OF_FOLDS\n",
    "with tqdm(total=total, desc = \"CV steps\"):\n",
    "    for lr in lr_optios:\n",
    "        for wd in weight_decay_optios:\n",
    "            df.loc[:, 'y_pred'] = -1\n",
    "            for fold in range(NUMBER_OF_FOLDS):\n",
    "                output_name = f'lr-{lr}_wd-{wd}'\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "                optimizer = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "                trainer = Trainer(data=df,\n",
    "                                tokenizer=tokenizer,\n",
    "                                model=model,\n",
    "                                optimizer=optimizer,\n",
    "                                fold = fold,\n",
    "                                early_stopping=2,\n",
    "                                batch_size=8,\n",
    "                                num_epochs=6,\n",
    "                                output_name=output_name)\n",
    "\n",
    "\n",
    "            \n",
    "                trainer.train()\n",
    "                trainer.final_eval()\n",
    "                df.loc[:, 'y_pred'] = trainer.data.loc[:, 'y_pred']\n",
    "            preds.loc[:, output_name] = df.loc[:, 'y_pred']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e903c692405e917c7fbf1a2efa503212707a7d4a30a85b3348439c72c8274b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('aml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
