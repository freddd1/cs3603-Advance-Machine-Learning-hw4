{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:<br> In the `q3/data` folder where you put all the files: train, test, etc.. <br> In the `q3/models` folder we will save all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have `data` and `models` folders\n",
    "if not os.path.isdir('q3/data'):\n",
    "    os.makedirs('q3/data')\n",
    "\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('q3/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape=(2682, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>845974102619906048</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Democrats are smiling in D.C. that the Freedom...</td>\n",
       "      <td>2017-03-26 15:21:58</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>846166053663191040</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>General Kelly is doing a great job at the bord...</td>\n",
       "      <td>2017-03-27 04:04:42</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>835814988686233601</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>The race for DNC Chairman was, of course, tota...</td>\n",
       "      <td>2017-02-26 13:33:16</td>\n",
       "      <td>android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>835817351178301440</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>For first time the failing @nytimes will take ...</td>\n",
       "      <td>2017-02-26 13:42:39</td>\n",
       "      <td>android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>835916511944523777</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Russia talk is FAKE NEWS put out by the Dems, ...</td>\n",
       "      <td>2017-02-26 20:16:41</td>\n",
       "      <td>android</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id           handle  \\\n",
       "0  845974102619906048  realDonaldTrump   \n",
       "1  846166053663191040  realDonaldTrump   \n",
       "2  835814988686233601  realDonaldTrump   \n",
       "3  835817351178301440  realDonaldTrump   \n",
       "4  835916511944523777  realDonaldTrump   \n",
       "\n",
       "                                               tweet                 date  \\\n",
       "0  Democrats are smiling in D.C. that the Freedom...  2017-03-26 15:21:58   \n",
       "1  General Kelly is doing a great job at the bord...  2017-03-27 04:04:42   \n",
       "2  The race for DNC Chairman was, of course, tota...  2017-02-26 13:33:16   \n",
       "3  For first time the failing @nytimes will take ...  2017-02-26 13:42:39   \n",
       "4  Russia talk is FAKE NEWS put out by the Dems, ...  2017-02-26 20:16:41   \n",
       "\n",
       "    device  \n",
       "0   iphone  \n",
       "1   iphone  \n",
       "2  android  \n",
       "3  android  \n",
       "4  android  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "df = pd.read_csv('q3/data/train_data_for_students.tsv', sep='\\t', header=None)\n",
    "df.columns = ['id', 'handle', 'tweet', 'date', 'device']\n",
    "print(f'{df.shape=}')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "android                                                                                1683\n",
       "iphone                                                                                  755\n",
       "<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>                      201\n",
       "<a href=\"http://www.twitter.com\" rel=\"nofollow\">Twitter for BlackBerry</a>               13\n",
       "<a href=\"https://about.twitter.com/products/tweetdeck\" rel=\"nofollow\">TweetDeck</a>       9\n",
       "<a href=\"http://twitter.com/#!/download/ipad\" rel=\"nofollow\">Twitter for iPad</a>         4\n",
       "<a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>                               3\n",
       "<a href=\"https://periscope.tv\" rel=\"nofollow\">Periscope.TV</a>                            2\n",
       "<a href=\"http://www.facebook.com/twitter\" rel=\"nofollow\">Facebook</a>                     1\n",
       "Name: device, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.device.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is issue with the label column. There are values that not iphone / android. WTF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "android    1683\n",
       "iphone      755\n",
       "Name: device, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[(df.device == 'iphone') | (df.device == 'android')]\n",
    "df.device.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add numric label column\n",
    "# android = 1\n",
    "# iphone = 0\n",
    "\n",
    "df['label'] = 0\n",
    "df.loc[df['device'] == 'android', 'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input df shape : (2438, 6)\n",
      "Number of folds: 3, total samples (after removing NaN): 2438\n",
      "fold: 0, num samples: 813\n",
      "fold: 1, num samples: 813\n",
      "fold: 2, num samples: 812\n"
     ]
    }
   ],
   "source": [
    "from q3.data_processing import create_folds\n",
    "\n",
    "# Using StratifiedKfold since the label is not that balanced\n",
    "\n",
    "NUMBER_OF_FOLDS = 3\n",
    "\n",
    "df = create_folds(df, label_name='device', num_folds=NUMBER_OF_FOLDS, seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q3.data_processing import preprocess\n",
    "\n",
    "# remove urls from tweets snice all hte urls with tweeter shortener\n",
    "\n",
    "df.tweet = df.tweet.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding HP\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from q3.model import Trainer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "if not os.path.isfile('q3/data/predictions.csv'):\n",
    "    lr_optios = [5e-5, 5e-4]\n",
    "    weight_decay_optios = [5e-3, 1e-2, 2e-2]\n",
    "    preds = df[['id', 'label', 'fold']]\n",
    "    total = len(lr_optios)*len(weight_decay_optios)*NUMBER_OF_FOLDS\n",
    "    with tqdm(total=total, desc = \"CV steps\") as pbar:\n",
    "        for lr in lr_optios:\n",
    "            for wd in weight_decay_optios:\n",
    "                df.loc[:, 'y_pred'] = -1\n",
    "                for fold in range(NUMBER_OF_FOLDS):\n",
    "                    output_name = f'lr-{lr}_wd-{wd}'\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "                    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "                    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "                    trainer = Trainer(data=df,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    model=model,\n",
    "                                    optimizer=optimizer,\n",
    "                                    fold = fold,\n",
    "                                    early_stopping=2,\n",
    "                                    batch_size=8,\n",
    "                                    num_epochs=6,\n",
    "                                    output_name=output_name)\n",
    "\n",
    "\n",
    "                \n",
    "                    trainer.train()\n",
    "                    trainer.final_eval()\n",
    "                    df.loc[:, 'y_pred'] = trainer.data.loc[:, 'y_pred']\n",
    "\n",
    "                    pbar.update(1)\n",
    "                preds.loc[:, output_name] = df.loc[:, 'y_pred']\n",
    "\n",
    "    preds.to_csv('q3/data/predictions.csv', index=False)\n",
    "\n",
    "else:\n",
    "    preds = pd.read_csv('q3/data/predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best lr: 5e-05 and wd: 0.01 -> eval_acc: 0.9065\n"
     ]
    }
   ],
   "source": [
    "cols = [c for c in preds.columns if c not in ['id','label', 'fold']]\n",
    "best_acc = 0\n",
    "for col in cols:\n",
    "    acc = sum(preds.label == preds[col]) / len(preds)\n",
    "    if acc >= best_acc:\n",
    "        best_acc = acc\n",
    "        best_col = col\n",
    "\n",
    "best_lr = float(best_col.split('_wd')[0].split('lr-')[1])\n",
    "best_wd = float(best_col.split('_wd-')[1])\n",
    "\n",
    "\n",
    "print(f'best lr: {best_lr} and wd: {best_wd} -> eval_acc: {best_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default-5e1d49eb6647d482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\itama\\.cache\\huggingface\\datasets\\csv\\default-5e1d49eb6647d482\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85d02d1b0594790a0cb6a28d9f3fcbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fbe17c4e394f81af4811bc01a90e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\itama\\.cache\\huggingface\\datasets\\csv\\default-5e1d49eb6647d482\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881f5cb6ed594bd4803c81feb4b0fb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a40b71e107b425d8e891efcb30b674f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2438 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_name = 'final_train'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr, weight_decay=best_wd)\n",
    "\n",
    "trainer = Trainer(data=df,\n",
    "                tokenizer=tokenizer,\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                fold = None,\n",
    "                early_stopping=3,\n",
    "                batch_size=8,\n",
    "                num_epochs=10,\n",
    "                output_name=output_name,\n",
    "                only_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we alrdeay saved a model it will load it otherwise it will train with the best HP\n",
    "if not os.path.isfile(f'q3/models/{output_name}'):\n",
    "    trainer.train()\n",
    "\n",
    "    saved_state = dict(\n",
    "                 model_state = trainer.model.state_dict(),\n",
    "                 test_loss = trainer.test_loss,\n",
    "                 test_acc = trainer.test_acc, \n",
    "            )\n",
    "\n",
    "    torch.save(saved_state, f'q3/models/{output_name}')\n",
    "else:\n",
    "    trainer.model.load_state_dict(torch.load(f'q3/models/{output_name}')['model_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(867, 866)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest = pd.read_csv('q3/data/test_data.tsv', sep='\\t', header=None)\n",
    "dtest.columns = ['id', 'handle', 'tweet', 'date']\n",
    "len(dtest.id), len(np.unique(dtest.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a duplicate value in our test set. <br>\n",
    "We will do a workaround to handle this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, c = np.unique(dtest.id, return_counts=True)\n",
    "for id, count in zip (v, c):\n",
    "    if count > 1:\n",
    "        break\n",
    "    \n",
    "dtest.loc[dtest.id == id, 'id'] = [11111, id]\n",
    "assert len(dtest.id) == len(np.unique(dtest.id)), 'there is no duplicate id in the dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4acaf8426f67bae1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\itama\\.cache\\huggingface\\datasets\\csv\\default-4acaf8426f67bae1\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af8dd1d389d472badcce0429915ef29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a450050e92154718a26bf0513bc53ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\itama\\.cache\\huggingface\\datasets\\csv\\default-4acaf8426f67bae1\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2454884590134b018e40c2cbe5a4afcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81d9559023a49fb98136c52b492d9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/867 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c646ca0a5f0041038935469a3a8e276a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval epoch:   0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.final_eval(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest.loc[:, 'device'] = trainer.test_data['y_pred']  # take the label from the trainer\n",
    "dtest.loc[:, 'device'] = dtest['device'].map({1: 'android', 0: 'iphone'})\n",
    "dtest.loc[dtest.id == id, 'id'] = id # return the duplicate id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the submision file\n",
    "dummy = pd.read_csv('q3/data/dummy_submission.tsv', sep='\\t', header=None)\n",
    "assert dummy.shape == dtest.shape\n",
    "dtest.to_csv('q3/data/submission.tsv', sep='\\t', header=None, index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2930d4c6478528e491fc50fad01bf56b72992349713b7eff7273f7199b740562"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('aml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
